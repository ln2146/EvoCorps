#!/usr/bin/env python3
"""
Multi-model selection system - centrally manages model selection for all agents.
Supports random selection across multiple models with fallback mechanisms.
"""

from __future__ import annotations

import random
import logging
import time
import asyncio
from typing import Any, Dict, Optional, Tuple, TYPE_CHECKING
from openai import OpenAI
import httpx
from keys import OPENAI_API_KEY, OPENAI_BASE_URL, EMBEDDING_API_KEY, EMBEDDING_BASE_URL

if TYPE_CHECKING:
    from langchain_openai import ChatOpenAI


class MultiModelSelector:
    """Multi-model selection system."""
    
    # Full list of supported models.
    ALL_MODELS = [
        "gpt-4.1-nano",      # OpenAI-compatible model
        "gemini-2.0-flash",  # Google Gemini model
        "deepseek-chat",
    ]

    # Centralized per-role model pools.
    #
    # Keep all role defaults here. If you want every role to use DeepSeek, keep the
    # pools as ["deepseek-chat"]. If you want multi-provider fallback, add models
    # to the lists below (they must be OpenAI-compatible on your OPENAI_BASE_URL).
    EMBEDDING_MODEL = "embedding-3"
    DEFAULT_POOL = ["deepseek-chat"]
    ROLE_MODEL_POOLS: dict[str, list[str]] = {
        # Core agent roles
        "regular": DEFAULT_POOL,
        "malicious": DEFAULT_POOL,
        "analyst": DEFAULT_POOL,
        "strategist": DEFAULT_POOL,
        "leader": DEFAULT_POOL,
        "echo": DEFAULT_POOL,
        # System roles
        "memory": DEFAULT_POOL,
        "fact_checker": DEFAULT_POOL,
        "summary": DEFAULT_POOL,
        "experiment": DEFAULT_POOL,
        "interview": DEFAULT_POOL,
        "comment_diversity": DEFAULT_POOL,
        # Embedding role (client only; embedding model is supplied by caller)
        "embedding": [EMBEDDING_MODEL],
    }

    # Backwards-compatible aliases used elsewhere in this repo.
    AVAILABLE_MODELS = ROLE_MODEL_POOLS["regular"]
    MALICIOUS_ECHO_MODELS = ROLE_MODEL_POOLS["malicious"]
    FALLBACK_PRIORITY = ROLE_MODEL_POOLS["regular"]
    MALICIOUS_ECHO_FALLBACK = ROLE_MODEL_POOLS["malicious"]
    
    def __init__(self):
        self.usage_stats = {model: 0 for model in self.ALL_MODELS}
        self.failure_stats = {model: 0 for model in self.ALL_MODELS}
        self.api_health_status = {}
        self.failed_api_cooldown = 300  # 5-minute cooldown
        self.last_failure_time = {}

        # New: request configuration optimized to align with utils.py
        self.request_config = {
            "timeout": 120,  # Increased timeout to match utils.py
            "max_retries": 1,  # Fewer retries to fail fast into fallback
            "retry_delay": 2,  # Retry delay
            "connection_pool_size": 5,  # Smaller connection pool to avoid conflicts
            "max_keepalive_connections": 2  # Lower keep-alive count
        }

        # New: request pacing control matching utils.py
        self.last_request_time = {model: 0 for model in self.ALL_MODELS}
        # Extended intervals to address persistent API errors
        self.min_request_interval = {
            "deepseek-chat": 10.0,    # DeepSeek can be rate-limited on some gateways
            "gemini-2.0-flash": 8.0,  # Gemini interval 8 seconds (increased)
            "gpt-4.1-nano": 6.0,      # GPT interval 6 seconds
            "default": 8.0            # Others 8 seconds (increased)
        }
        self.request_lock = {model: False for model in self.ALL_MODELS}  # Add per-model lock

        # New: connection pool management
        self._http_client = None
        
    def _wait_for_request_interval(self, model: str):
        """ç­‰å¾…è¯·æ±‚é—´éš”ï¼Œé¿å…è¿‡äºé¢‘ç¹çš„è¯·æ±‚"""
        # checkæ¨¡å‹æ˜¯å¦è¢«é”å®š
        max_wait_attempts = 10
        wait_attempts = 0

        while self.request_lock.get(model, False) and wait_attempts < max_wait_attempts:
            time.sleep(0.1)
            wait_attempts += 1

        # é”å®šæ¨¡å‹
        self.request_lock[model] = True

        try:
            current_time = time.time()
            last_request = self.last_request_time.get(model, 0)
            time_since_last = current_time - last_request

            # æ ¹æ®failedæ¬¡æ•°dynamicadjusté—´éš”
            failure_count = self.failure_stats.get(model, 0)
            # getæ¨¡å‹ç‰¹å®šçš„é—´éš”æ—¶é—´
            base_interval = self.min_request_interval.get(model, self.min_request_interval["default"])
            dynamic_interval = base_interval

            if failure_count >= 3:
                dynamic_interval = base_interval * (1 + failure_count * 0.5)

            if time_since_last < dynamic_interval:
                wait_time = dynamic_interval - time_since_last
                if wait_time > 0.1:  # åªæœ‰ç­‰å¾…æ—¶é—´è¶…è¿‡0.1ç§’æ‰ç­‰å¾…
                    time.sleep(wait_time)

            self.last_request_time[model] = time.time()
        finally:
            # é‡Šæ”¾é”
            self.request_lock[model] = False

    @staticmethod
    def _normalize_role(role: Optional[str]) -> str:
        if not role:
            return "regular"
        role_norm = str(role).strip().lower()
        return role_norm or "regular"

    def get_model_pool(self, role: Optional[str] = None) -> list[str]:
        """Get model pool for a role; falls back to regular."""
        role_norm = self._normalize_role(role)
        pool = self.ROLE_MODEL_POOLS.get(role_norm)
        return pool if pool else self.ROLE_MODEL_POOLS["regular"]

    def select_random_model(self, role: Optional[str] = None) -> str:
        """randomselectä¸€ä¸ªæ¨¡å‹ï¼ˆæŒ‰è§’è‰²ï¼‰"""
        pool = self.get_model_pool(role)
        # ä¼˜å…ˆselectå¥åº·çš„æ¨¡å‹
        healthy_models = [model for model in pool if self.is_model_healthy(model)]

        if healthy_models:
            selected = random.choice(healthy_models)
        else:
            # å¦‚æœæ²¡æœ‰å¥åº·çš„æ¨¡å‹ï¼Œselectå†·å´æ—¶é—´æœ€çŸ­çš„
            selected = min(pool, key=lambda m: self.last_failure_time.get(m, 0))
            print(f"âš ï¸ æ‰€æœ‰æ¨¡å‹éƒ½åœ¨å†·å´æœŸï¼Œé€‰æ‹©å†·å´æ—¶é—´æœ€çŸ­çš„: {selected}")

        self.usage_stats[selected] += 1
        return selected
    
    def select_model_for_agent_type(self, agent_type: str) -> str:
        """æ ¹æ®Agentclasså‹selectæ¨¡å‹ï¼ˆå…¼å®¹æ—§è°ƒç”¨ï¼‰"""
        return self.select_random_model(role=agent_type)

    def is_model_healthy(self, model: str) -> bool:
        """checkæ¨¡å‹æ˜¯å¦å¥åº·ï¼ˆæœªåœ¨å†·å´æœŸï¼‰"""
        import time
        if model not in self.last_failure_time:
            return True

        time_since_failure = time.time() - self.last_failure_time[model]
        return time_since_failure > self.failed_api_cooldown

    def mark_model_failed(self, model: str, error_type: str = "unknown"):
        """æ ‡è®°æ¨¡å‹failed"""
        import time
        if model not in self.failure_stats:
            self.failure_stats[model] = 0
            self.usage_stats[model] = 0
        self.failure_stats[model] += 1
        self.last_failure_time[model] = time.time()
        print(f"ğŸš« æ¨¡å‹ {model} æ ‡è®°ä¸ºå¤±è´¥ï¼Œè¿›å…¥å†·å´æœŸ")
        print(f"âš ï¸ æ¨¡å‹ {model} å¤±è´¥æ¬¡æ•°: {self.failure_stats[model]}")

        # æ ¹æ®errorclasså‹æä¾›è¯¦ç»†information
        if "502" in error_type or "Bad Gateway" in error_type:
            print(f"âš ï¸ æ¨¡å‹ {model} é‡åˆ°502é”™è¯¯ï¼Œæ ‡è®°ä¸ºå¤±è´¥å¹¶å°è¯•ä¸‹ä¸€ä¸ªæ¨¡å‹...")
        elif "405" in error_type or "bad_response_status_code" in error_type:
            print(f"âš ï¸ æ¨¡å‹ {model} é‡åˆ°405é”™è¯¯ï¼Œæ–¹æ³•ä¸æ”¯æŒï¼Œæ ‡è®°ä¸ºå¤±è´¥...")
        elif "content_filter" in error_type:
            print(f"âš ï¸ æ¨¡å‹ {model} é‡åˆ°å†…å®¹è¿‡æ»¤é”™è¯¯ï¼Œå†…å®¹è¢«Microsoftç­–ç•¥è¿‡æ»¤ï¼Œæ ‡è®°ä¸ºå¤±è´¥...")
        elif "400" in error_type:
            print(f"âš ï¸ æ¨¡å‹ {model} é‡åˆ°400é”™è¯¯ï¼Œè¯·æ±‚æ ¼å¼é—®é¢˜ï¼Œæ ‡è®°ä¸ºå¤±è´¥...")
        elif "timeout" in error_type.lower():
            print(f"âš ï¸ æ¨¡å‹ {model} é‡åˆ°è¶…æ—¶é”™è¯¯ï¼Œæ ‡è®°ä¸ºå¤±è´¥...")
        elif "rate_limit" in error_type.lower():
            print(f"âš ï¸ æ¨¡å‹ {model} é‡åˆ°é€Ÿç‡é™åˆ¶ï¼Œæ ‡è®°ä¸ºå¤±è´¥...")
        elif "upstream_error" in error_type:
            print(f"âš ï¸ æ¨¡å‹ {model} é‡åˆ°ä¸Šæ¸¸é”™è¯¯ï¼Œæ ‡è®°ä¸ºå¤±è´¥...")
        elif "json_parse_error" in error_type:
            print(f"âš ï¸ æ¨¡å‹ {model} è¿”å›ç©ºå“åº”æˆ–æ ¼å¼é”™è¯¯ï¼Œæ ‡è®°ä¸ºå¤±è´¥...")

        return None

    def handle_api_error(self, model: str, error: Exception) -> str:
        """processAPIerrorå¹¶è¿”å›å›é€€æ¨¡å‹"""
        error_str = str(error)

        # identifyerrorclasså‹
        if "502" in error_str or "Bad Gateway" in error_str:
            error_type = "502_bad_gateway"
        elif "405" in error_str or "bad_response_status_code" in error_str:
            error_type = "405_error"
        elif "400" in error_str and "content_filter" in error_str.lower():
            error_type = "content_filter"
        elif "400" in error_str:
            error_type = "400_error"
        elif "upstream_error" in error_str:
            error_type = "upstream_error"
        elif "timeout" in error_str.lower():
            error_type = "timeout"
        elif "rate_limit" in error_str.lower():
            error_type = "rate_limit"
        elif "Expecting value" in error_str and "char 0" in error_str:
            error_type = "json_parse_error"
        else:
            error_type = "unknown"

        # æ ‡è®°æ¨¡å‹failed
        self.mark_model_failed(model, error_type)

        # getä¸‹ä¸€ä¸ªå¯ç”¨æ¨¡å‹
        next_model = self.get_next_fallback_model(model)
        if next_model:
            print(f"âœ… æ¨¡å‹å›é€€æˆåŠŸ: {model} -> {next_model}")
            return next_model
        else:
            print(f"âŒ æ²¡æœ‰å¯ç”¨çš„å›é€€æ¨¡å‹")
            return None

    def get_healthy_models(self) -> list[str]:
        """getå¥åº·çš„æ¨¡å‹åˆ—è¡¨ï¼ˆregular æ± ï¼‰"""
        pool = self.get_model_pool("regular")
        return [model for model in pool if self.is_model_healthy(model)]

    def get_next_fallback_model(self, failed_model: str) -> str:
        """getä¸‹ä¸€ä¸ªå›é€€æ¨¡å‹"""
        try:
            current_index = self.FALLBACK_PRIORITY.index(failed_model)
            # è¿”å›ä¸‹ä¸€ä¸ªä¼˜å…ˆçº§çš„æ¨¡å‹
            if current_index + 1 < len(self.FALLBACK_PRIORITY):
                return self.FALLBACK_PRIORITY[current_index + 1]
            else:
                # å¦‚æœå·²ç»æ˜¯æœ€åä¸€ä¸ªï¼Œè¿”å›ç¬¬ä¸€ä¸ª
                return self.FALLBACK_PRIORITY[0]
        except ValueError:
            # å¦‚æœæ¨¡å‹ä¸åœ¨ä¼˜å…ˆçº§åˆ—è¡¨ä¸­ï¼Œè¿”å›defaultæ¨¡å‹
            return self.FALLBACK_PRIORITY[0]

    def select_smart_model(self) -> str:
        """æ™ºèƒ½selectæ¨¡å‹ - ä¼˜å…ˆselectå¥åº·çš„æ¨¡å‹"""
        healthy_models = self.get_healthy_models()

        if not healthy_models:
            print("âš ï¸ æ‰€æœ‰æ¨¡å‹éƒ½åœ¨å†·å´æœŸï¼Œä½¿ç”¨æœ€æ—©å¤±è´¥çš„æ¨¡å‹")
            # selectæœ€æ—©failedçš„æ¨¡å‹ï¼ˆå†·å´æ—¶é—´æœ€é•¿ï¼‰
            if self.last_failure_time:
                return min(self.last_failure_time.keys(), key=lambda x: self.last_failure_time[x])
            else:
                return random.choice(self.get_model_pool("regular"))

        # ä»å¥åº·æ¨¡å‹ä¸­randomselect
        selected = random.choice(healthy_models)
        self.usage_stats[selected] += 1
        return selected

    def select_malicious_echo_model(self) -> str:
        """ä¸ºæ¶æ„æ°´å†›å’Œecho_groupselectæ¨¡å‹"""
        # getå¥åº·çš„æ¶æ„æ°´å†›ä¸“ç”¨æ¨¡å‹
        pool = self.get_model_pool("echo")
        healthy_malicious_models = [model for model in pool if self.is_model_healthy(model)]

        if not healthy_malicious_models:
            print("âš ï¸ æ‰€æœ‰æ¶æ„æ°´å†›ä¸“ç”¨æ¨¡å‹éƒ½åœ¨å†·å´æœŸï¼Œä½¿ç”¨æœ€æ—©å¤±è´¥çš„æ¨¡å‹")
            # ä»æ¶æ„æ°´å†›ä¸“ç”¨æ¨¡å‹ä¸­selectæœ€æ—©failedçš„
            malicious_failure_times = {k: v for k, v in self.last_failure_time.items() if k in pool}
            if malicious_failure_times:
                return min(malicious_failure_times.keys(), key=lambda x: malicious_failure_times[x])
            else:
                return random.choice(pool)

        # ä»å¥åº·çš„æ¶æ„æ°´å†›ä¸“ç”¨æ¨¡å‹ä¸­randomselect
        selected = random.choice(healthy_malicious_models)
        self.usage_stats[selected] += 1
        return selected

    def create_openai_client(self, model_name: str = None, role: str = "regular") -> Tuple[OpenAI, str]:
        """createoptimizeçš„OpenAIclient"""
        if model_name is None:
            model_name = self.select_random_model(role=role)

        # è¯·æ±‚é—´éš”æ§åˆ¶
        self._wait_for_request_interval(model_name)

        # createoptimizeçš„HTTPclient
        http_client = httpx.Client(
            timeout=self.request_config["timeout"],
            limits=httpx.Limits(
                max_connections=self.request_config["connection_pool_size"],
                max_keepalive_connections=self.request_config["max_keepalive_connections"]
            )
        )

        client = OpenAI(
            api_key=OPENAI_API_KEY,
            base_url=OPENAI_BASE_URL,
            timeout=self.request_config["timeout"],
            http_client=http_client
        )

        return client, model_name

    def create_openai_client_with_base_url(
        self,
        base_url: str,
        api_key: str,
        model_name: str = None,
        role: str = "regular",
    ) -> Tuple[OpenAI, str]:
        """Create OpenAI client with a custom base_url (e.g., local/ollama)."""
        if model_name is None:
            model_name = self.select_random_model(role=role)

        # è¯·æ±‚é—´éš”æ§åˆ¶
        self._wait_for_request_interval(model_name)

        http_client = httpx.Client(
            timeout=self.request_config["timeout"],
            limits=httpx.Limits(
                max_connections=self.request_config["connection_pool_size"],
                max_keepalive_connections=self.request_config["max_keepalive_connections"]
            )
        )

        client = OpenAI(
            api_key=api_key,
            base_url=base_url,
            timeout=self.request_config["timeout"],
            http_client=http_client
        )

        return client, model_name

    def create_embedding_client(self, model_name: str = None) -> Tuple[OpenAI, str]:
        """Create OpenAI client for embeddings."""
        if model_name is None:
            model_name = self.EMBEDDING_MODEL
        return self.create_openai_client_with_base_url(
            base_url=EMBEDDING_BASE_URL,
            api_key=EMBEDDING_API_KEY,
            model_name=model_name,
            role="embedding",
        )
    
    def create_langchain_client(self, model_name: str = None, role: str = "regular", **kwargs) -> Tuple[ChatOpenAI, str]:
        """createoptimizeçš„LangChain ChatOpenAIclient"""
        try:
            from langchain_openai import ChatOpenAI
        except ModuleNotFoundError as e:
            raise ModuleNotFoundError(
                "Missing dependency 'langchain-openai'. Install it or avoid calling create_langchain_client()."
            ) from e

        if model_name is None:
            model_name = self.select_random_model(role=role)

        # è¯·æ±‚é—´éš”æ§åˆ¶
        self._wait_for_request_interval(model_name)

        # getsecureçš„æ¨¡å‹configureï¼ˆé¿å…ä¸æ”¯æŒçš„parameterï¼‰
        model_config = self.get_safe_model_config(model_name)

        # mergeuseræä¾›çš„parameter
        final_config = model_config.copy()
        final_config.update(kwargs)

        # createoptimizeçš„HTTPclient
        http_client = httpx.Client(
            timeout=self.request_config["timeout"],
            limits=httpx.Limits(
                max_connections=self.request_config["connection_pool_size"],
                max_keepalive_connections=self.request_config["max_keepalive_connections"]
            )
        )

        client = ChatOpenAI(
            model=model_name,
            openai_api_key=OPENAI_API_KEY,
            openai_api_base=OPENAI_BASE_URL,
            timeout=self.request_config["timeout"],
            max_retries=self.request_config["max_retries"],
            http_client=http_client,
            **final_config
        )

        return client, model_name
    
    def create_client_with_fallback(
        self,
        preferred_model: str = None,
        client_type: str = "openai",
        role: str = "regular",
    ) -> Tuple[Any, str]:
        """createclientï¼Œæ”¯æŒå›é€€æœºåˆ¶ï¼ˆæŒ‰è§’è‰²ï¼‰"""
        fallback_pool = self.get_model_pool(role)
        models_to_try = fallback_pool.copy() if preferred_model is None else [preferred_model] + [m for m in fallback_pool if m != preferred_model]
        
        last_error = None
        for model in models_to_try:
            try:
                if client_type == "langchain":
                    client, selected_model = self.create_langchain_client(model, role=role)
                else:
                    client, selected_model = self.create_openai_client(model, role=role)
                
                # simpletestingconnectï¼ˆå¯é€‰ï¼‰
                return client, selected_model
                
            except Exception as e:
                last_error = e
                self.failure_stats[model] += 1
                logging.warning(f"æ¨¡å‹ {model} connectfailed: {e}")
                continue
        
        # å¦‚æœæ‰€æœ‰æ¨¡å‹éƒ½failedï¼ŒæŠ›å‡ºexception
        raise Exception(f"æ‰€æœ‰æ¨¡å‹éƒ½connectfailedï¼Œæœ€åerror: {last_error}")
    
    def get_model_config(self, model_name: str) -> Dict[str, Any]:
        """getç‰¹å®šæ¨¡å‹çš„configureparameter"""
        # basicconfigureï¼ˆæ‰€æœ‰æ¨¡å‹éƒ½æ”¯æŒï¼‰
        base_configs = {
            "gpt-4.1-nano": {
                "temperature": 0.8,
                "max_tokens": 150
            },
            "deepseek-chat": {
                "temperature": 0.7,
                "max_tokens": 200
            },
            "gemini-2.0-flash": {
                "temperature": 0.75,
                "max_tokens": 180
            },

        }

        # æ¨¡å‹ç‰¹å®šçš„æ”¯æŒparameter
        model_specific_params = {
            "gpt-4.1-nano": {
                "frequency_penalty": 0.3,
                "presence_penalty": 0.3
            },
            "deepseek-chat": {},
            "gemini-2.0-flash": {},
        }

        # getbasicconfigureï¼Œå¦‚æœæ¨¡å‹ä¸å­˜åœ¨åˆ™æŠ›å‡ºexception
        if model_name not in base_configs:
            raise ValueError(f"æ¨¡å‹ {model_name} ä¸æ”¯æŒ")
        config = base_configs[model_name].copy()

        # æ·»åŠ æ¨¡å‹ç‰¹å®šparameter
        if model_name in model_specific_params:
            config.update(model_specific_params[model_name])

        return config

    def get_safe_model_config(self, model_name: str) -> Dict[str, Any]:
        """getsecureçš„æ¨¡å‹configureparameterï¼ˆç§»é™¤å¯èƒ½ä¸æ”¯æŒçš„parameterï¼‰"""
        config = self.get_model_config(model_name)

        # å®šä¹‰secureçš„é€šç”¨parameterï¼ˆå¤§å¤šæ•°æ¨¡å‹éƒ½æ”¯æŒï¼‰
        safe_params = ["temperature", "max_tokens", "top_p", "top_k"]

        # createsecureconfigure
        safe_config = {}
        for param in safe_params:
            if param in config:
                safe_config[param] = config[param]

        # å¯¹äºç‰¹å®šæ¨¡å‹ï¼Œè°¨æ…æ·»åŠ advancedparameter
        if model_name == "gpt-4.1-nano":
            # GPTæ¨¡å‹é€šå¸¸æ”¯æŒpenaltyparameter
            if "frequency_penalty" in config:
                safe_config["frequency_penalty"] = config["frequency_penalty"]
            if "presence_penalty" in config:
                safe_config["presence_penalty"] = config["presence_penalty"]

        return safe_config
    
    def get_usage_stats(self) -> Dict[str, Any]:
        """getusingstatistics"""
        total_usage = sum(self.usage_stats.values())
        total_failures = sum(self.failure_stats.values())
        
        return {
            "total_requests": total_usage,
            "total_failures": total_failures,
            "success_rate": (total_usage - total_failures) / max(total_usage, 1),
            "model_usage": self.usage_stats.copy(),
            "model_failures": self.failure_stats.copy(),
            "model_success_rates": {
                model: (self.usage_stats[model] - self.failure_stats[model]) / max(self.usage_stats[model], 1)
                for model in self.AVAILABLE_MODELS
            }
        }
    
    def print_stats(self):
        """æ‰“å°usingstatistics"""
        stats = self.get_usage_stats()
        print("\nğŸ¤– å¤šæ¨¡å‹é€‰æ‹©ç³»ç»Ÿç»Ÿè®¡:")
        print(f"   æ€»è¯·æ±‚æ•°: {stats['total_requests']}")
        print(f"   æ€»å¤±è´¥æ•°: {stats['total_failures']}")
        print(f"   æ€»æˆåŠŸç‡: {stats['success_rate']:.1%}")
        print("\nğŸ“Š å„æ¨¡å‹ä½¿ç”¨æƒ…å†µ:")
        for model in self.AVAILABLE_MODELS:
            usage = stats['model_usage'][model]
            failures = stats['model_failures'][model]
            success_rate = stats['model_success_rates'][model]
            print(f"   {model}: {usage}æ¬¡ä½¿ç”¨, {failures}æ¬¡å¤±è´¥, {success_rate:.1%}æˆåŠŸç‡")

    def record_usage(self, model_name: str, success: bool = True):
        """recordæ¨¡å‹usingæƒ…å†µ"""
        try:
            if model_name in self.ALL_MODELS:
                self.usage_stats[model_name] += 1
                if not success:
                    self.failure_stats[model_name] += 1
                    # Align with is_model_healthy(): failures mark last_failure_time.
                    self.last_failure_time[model_name] = time.time()
            else:
                print(f"âš ï¸ æœªçŸ¥æ¨¡å‹: {model_name}")
        except Exception as e:
            print(f"âš ï¸ è®°å½•æ¨¡å‹ä½¿ç”¨å¤±è´¥: {e}")


# globalå¤šæ¨¡å‹selectå™¨instance
multi_model_selector = MultiModelSelector()


def get_random_model(role: str = "regular") -> str:
    """getrandomæ¨¡å‹çš„ä¾¿æ·å‡½æ•°ï¼ˆæŒ‰è§’è‰²ï¼‰"""
    return multi_model_selector.select_random_model(role=role)


def create_model_client(agent_type: str = "normal", client_type: str = "openai", **kwargs) -> Tuple[Any, str]:
    """ä¸ºAgentcreateæ¨¡å‹clientçš„ä¾¿æ·å‡½æ•°"""
    try:
        return multi_model_selector.create_client_with_fallback(client_type=client_type, role=agent_type, **kwargs)
    except Exception as e:
        logging.error(f"create{agent_type} Agentçš„æ¨¡å‹clientfailed: {e}")
        raise


def log_model_usage(agent_type: str, model_name: str, success: bool = True):
    """recordæ¨¡å‹usingæƒ…å†µ"""
    if not success:
        multi_model_selector.failure_stats[model_name] += 1
    
    logging.info(f"{agent_type} Agentusingæ¨¡å‹: {model_name}, successful: {success}")


if __name__ == "__main__":
    # testingå¤šæ¨¡å‹selectsystem
    print("ğŸ§ª æµ‹è¯•å¤šæ¨¡å‹é€‰æ‹©ç³»ç»Ÿ...")
    
    selector = MultiModelSelector()
    
    # testingrandomselect
    print("\nğŸ² éšæœºæ¨¡å‹é€‰æ‹©æµ‹è¯•:")
    for i in range(10):
        model = selector.select_random_model()
        print(f"   é€‰æ‹© {i+1}: {model}")
    
    # æ‰“å°statistics
    selector.print_stats()
    
    # testingclientcreate
    print("\nğŸ”§ å®¢æˆ·ç«¯åˆ›å»ºæµ‹è¯•:")
    try:
        client, model = selector.create_openai_client()
        print(f"   âœ… OpenAIå®¢æˆ·ç«¯åˆ›å»ºæˆåŠŸï¼Œæ¨¡å‹: {model}")
    except Exception as e:
        print(f"   âŒ OpenAIå®¢æˆ·ç«¯åˆ›å»ºå¤±è´¥: {e}")
    
    try:
        client, model = selector.create_langchain_client()
        print(f"   âœ… LangChainå®¢æˆ·ç«¯åˆ›å»ºæˆåŠŸï¼Œæ¨¡å‹: {model}")
    except Exception as e:
        print(f"   âŒ LangChainå®¢æˆ·ç«¯åˆ›å»ºå¤±è´¥: {e}")
